{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN - Tutorial 2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"FlsNuQbbRxmo","colab_type":"text"},"cell_type":"markdown","source":["# Standard Neural Networks Tutorial 2\n","\n","This tutorial's aim is to show how to create a custom layer. It uses the MNIST dataset.\n","\n","We will first create a layer that we will call __Linear__ which is the same as the Dense one i.e. calculates __*y = wx + b*__  \n"]},{"metadata":{"id":"jREI9uNlSCLP","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","import datetime\n","\n","from tensorflow.keras.datasets.mnist import load_data\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Layer, Activation, Input, Flatten\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import imshow\n","\n","from PIL import Image, ImageOps"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MyWmmkHTSXf-","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"TensorFlow version: \" + str(tf.__version__))\n","print(\"Keras version: \" + str(tf.keras.__version__))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AnGsyINmSwxM","colab_type":"text"},"cell_type":"markdown","source":["# Loading and Preprocessing\n","\n","Let's start by loading the data from Keras dataset\n","\n","The dataset is composed of:\n","\n","\n","*   60000 images for training.\n","*   10000 images for testing\n","\n","Each image is a grayscale one of size 28x28.  \n","Instead of changing the dimensions to make each image a vector, we will keep the data as it is and add a __Flatten__ layer at the beginning of the model that will do the same job.\n","Data dimension will be:  \n","* X: (?, 28, 28)\n","*   Y: (?, )"]},{"metadata":{"id":"L-H2KvCQS-hc","colab_type":"code","colab":{}},"cell_type":"code","source":["# Reading datasets\n","(x_train, y_train), (x_test, y_test) = load_data()\n","\n","\n","x_train = x_train / 255\n","x_test  = x_test / 255\n","\n","print(\"X Train dimensions: \" + str(x_train.shape))\n","print(\"Y Train dimensions: \" + str(y_train.shape))\n","\n","print(\"X Test dimensions: \" + str(x_test.shape))\n","print(\"Y Test dimensions: \" + str(y_test.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lo68FL2MjZef","colab_type":"code","colab":{}},"cell_type":"code","source":["# Printing a sample image\n","i=np.random.randint(0, x_train.shape[0] - 1)\n","plt.title('Title: Plotting image:%d .... Value is %d' % (i,y_train[i]))\n","plt.imshow(x_train[i], cmap='gray')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k4BGNFnP73uD","colab_type":"text"},"cell_type":"markdown","source":["# Custom Layer\n","\n","Let's create a custom layer that we will call __Linear__ which implements the same functionality as Dense i.e. __*y = wx + b*__  \n","\n","Each custom layer must inherit the Layer Class defined in tensorflow.keras.layers and must have the following functions:\n","* **\\__init__** which initialises the layer\n","* **build** in which we define the parameters\n","* **call** which does the layer forward propagation\n","* **compute_output_shape** which computes the shape of the output tensor given the input tensor shape\n","\n","There are other functions in case we need to save and load the model"]},{"metadata":{"id":"g64RhFChkID-","colab_type":"code","colab":{}},"cell_type":"code","source":["class Linear(Layer):\n","  def __init__(self, units, **kwargs):\n","    self.units = units\n","    super(Linear, self).__init__(**kwargs)\n","\n","  def build(self, input_shape):\n","    shape = tf.TensorShape((input_shape[-1], self.units)).as_list()\n","    self.w = self.add_weight(name='kernel',\n","                             shape=shape,\n","                             initializer='glorot_uniform',\n","                             trainable=True)\n","    self.b = self.add_weight(name='bias',\n","                             shape=(self.units,),\n","                             initializer='zeros',\n","                             trainable=True)\n","    super(Linear, self).build(input_shape)\n","  \n","  def call(self, inputs):\n","    output = tf.matmul(inputs, self.w) + self.b\n","    return output\n","\n","  def compute_output_shape(self, input_shape):\n","    shape = tf.TensorShape(input_shape).as_list()\n","    shape[-1] = self.units\n","    return tf.TensorShape(shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GqbvmvmVINxA","colab_type":"text"},"cell_type":"markdown","source":["# Model\n","\n","\n","The first model is the same as the one we saw earlier, but instead of using Keras Dense layer, we will use the Linear layer that we defined earlier.  \n","Here we will usehe Functional Model which always starts with an __Input__ layer.  \n","__Note:__ This model will have the same number of parameters as if we used the Keras __Dense__ Layer"]},{"metadata":{"id":"-ZRCdvPLWZHi","colab_type":"code","colab":{}},"cell_type":"code","source":["X = Input(shape=x_train.shape[1:])\n","\n","Y = Flatten()(X)\n","Y = Linear(100)(Y)\n","Y = Activation('relu')(Y)\n","Y = Linear(10)(Y)\n","Y = Activation('softmax')(Y)\n","\n","model = Model(inputs = X, outputs = Y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2UEGDmJbJabc","colab_type":"text"},"cell_type":"markdown","source":["# Compilation\n","\n","Next we need to compile the model by calling __model.compile()__ and specifying the following:\n","* __Optimizer__ We can choose stochastic gradient descent __SGD__ or any other more powerful optimisation method like __Adam__\n","* __Loss function__ The sparse_categorical_crossentropy\n","* __Metrics__ to use while training. Here the accuracy between the real y and the predicted one  \n","\n","__model.summary()__ summarises the model showing the layers along with their parameters"]},{"metadata":{"id":"aipP6lqlZbeo","colab_type":"code","colab":{}},"cell_type":"code","source":["model.compile(optimizer=\"Adam\", loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d4ABQaszJ7jL","colab_type":"text"},"cell_type":"markdown","source":["# Model Training\n","\n","We train the model by calling __model.fit()__ and giving it the training x_train and y_train  \n","We can also validate the model by giving the x_test and y_test\n","\n","The model will be trained and the parameters will be updated based on x_train and y_train **only**. x_test and y_test will be used to evaluate how well the model is doing on the validation set"]},{"metadata":{"id":"_JXQHVKzZ2kr","colab_type":"code","colab":{}},"cell_type":"code","source":["validation = True\n","if validation == True:\n","  eval_data = (x_test, y_test)\n","else:\n","  eval_data = None\n","\n","history = model.fit(x =x_train, y = y_train, epochs=30, batch_size=1024, validation_data=eval_data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oBsk22JeMRrH","colab_type":"text"},"cell_type":"markdown","source":["# Training Results"]},{"metadata":{"id":"P5Wx9MnZaJ7I","colab_type":"code","colab":{}},"cell_type":"code","source":["print (\"Training Accuracy = %.4f\"  % (history.history['acc'][-1]))\n","print (\"Training Loss = %.4f\"  % (history.history['loss'][-1]))\n","\n","if validation is True:\n","  print (\"Validation Accuracy = %.4f\"  % (history.history['val_acc'][-1]))\n","  print (\"Validation Loss = %.4f\"  % (history.history['val_loss'][-1]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b4rN3GmE4UrD","colab_type":"text"},"cell_type":"markdown","source":["# Plotting Results\n","\n","Here we plot the training and evaluation metrics' history.\n","2 Plots:\n","* One for loss\n","* One for accuracy"]},{"metadata":{"id":"CxZhLOlxdJTe","colab_type":"code","colab":{}},"cell_type":"code","source":["  plt.figure(figsize=(12,5))\n","  plt.subplot(1, 2, 1)\n","  plt.plot(history.history['acc'], 'b', label='Training acc')\n","  if validation is True:\n","    plt.plot(history.history['val_acc'], 'r', label='Validating acc')\n","  plt.title('Model Accuracy')\n","  plt.ylabel('Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend()\n","\n","  plt.subplot(1, 2, 2)\n","  plt.plot(history.history['loss'], 'b', label=\"Training loss\")\n","  if validation is True:\n","    plt.plot(history.history['val_loss'], 'r', label=\"Validating loss\")\n","  plt.title('Model Loss')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.legend()\n","  \n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dyqdumjeDB9O","colab_type":"text"},"cell_type":"markdown","source":["# Early Stopping\n","\n","The number of epochs to run a model is very crutial.   \n","Sometimes, we select a small number of epochs which leads to high loss and poor accuracy and some other time we choose a large number of epochs, reach the accuracy that we want but sill have to wait for it to finish.  \n","Wouldn't be nice if there is a way to stop training on a condition? Well there is using Keras Callbacks.  \n","Let's create a subclass of Callback which stops training where acc reaches 99%"]},{"metadata":{"id":"RuxkcnScBBif","colab_type":"code","colab":{}},"cell_type":"code","source":["class MyCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs={}):\n","    if logs.get('acc') > 0.99:\n","      print(\"\\nReached 99% accuracy ... Stopping the training!\")\n","      self.model.stop_training = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4wVD1gL4B06P","colab_type":"code","colab":{}},"cell_type":"code","source":["callbacks = MyCallback()\n","\n","m = Sequential()\n","m.add(Flatten(input_shape=x_train.shape[1:]))\n","m.add(Linear(100))\n","m.add(Activation('relu'))\n","m.add(Linear(10))\n","m.add(Activation('softmax'))\n","\n","m.compile(optimizer=\"Adam\", loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n","\n","m.fit(x =x_train, y = y_train, epochs=100, batch_size=1024, validation_data=(x_test, y_test), callbacks=[callbacks])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CSdQb3Ofj6ie","colab_type":"text"},"cell_type":"markdown","source":["# Summary\n","\n","In this tutorial, we learnt how to:\n","* Create a custom layer\n","* Use Keras Functional Model\n","* Create a Callback to stop a model's training when accuracy reaches a desired value"]}]}